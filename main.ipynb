{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import svm\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn import tree\n",
    "\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load in the data from csv (put dataset in same directory as this file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(891, 12)\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv(\"train.csv\")\n",
    "print(data.shape) # (891, 12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to split data into k folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for splitting dataset into k folds\n",
    "def split_data(y, folds):\n",
    "    # split data into k equal (as possible) sized sets\n",
    "    np.random.seed(0)\n",
    "    L = np.size(y)\n",
    "    order = np.random.permutation(L)\n",
    "    c = np.ones((L,1))\n",
    "    L = int(np.round((1/folds)*L))\n",
    "    for s in np.arange(0,folds-2):\n",
    "        c[order[s*L:(s+1)*L]] = s + 2\n",
    "    c[order[(folds-1)*L:]] = folds\n",
    "    return c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute eigendecomposition of data covariance matrix for PCA transformation\n",
    "def PCA(x,**kwargs):\n",
    "\t# regularization parameter for numerical stability\n",
    "\tlam = 10**(-7)\n",
    "\tif 'lam' in kwargs:\n",
    "\t\tlam = kwargs['lam']\n",
    "\n",
    "\t# create the correlation matrix\n",
    "\tP = float(x.shape[1])\n",
    "\tCov = 1/P*np.dot(x,x.T) + lam*np.eye(x.shape[0])\n",
    "\n",
    "\t# use numpy function to compute eigenvalues / vectors of correlation matrix\n",
    "\td,V = np.linalg.eigh(Cov)\n",
    "\treturn d,V\n",
    "\n",
    "# PCA-sphereing - use PCA to normalize input features\n",
    "def PCA_sphereing(x,**kwargs):\n",
    "\t# Step 1: mean-center the data\n",
    "\tx_means = np.mean(x, axis=1, dtype=float)[:,np.newaxis]\n",
    "\tx_centered = x - x_means\n",
    "\n",
    "\t# Step 2: compute pca transform on mean-centered data\n",
    "\td,V = PCA(x_centered,**kwargs)\n",
    "\n",
    "\t# Step 3: divide off standard deviation of each (transformed) input,\n",
    "\t# which are equal to the returned eigenvalues in 'd'.\n",
    "\tstds = (d[:,np.newaxis])**(0.5)\n",
    "\n",
    "\t# check to make sure thta x_stds > small threshold, for those not\n",
    "\t# divide by 1 instead of original standard deviation\n",
    "\tind = np.argwhere(stds < 10**(-2))\n",
    "\tif len(ind) > 0:\n",
    "\t\tind = [v[0] for v in ind]\n",
    "\t\tadjust = np.zeros((stds.shape))\n",
    "\t\tadjust[ind] = 1.0\n",
    "\t\tstds += adjust\n",
    "\n",
    "\tnormalizer = lambda data: np.dot(V.T,data - x_means)/stds\n",
    "\n",
    "\t# create inverse normalizer\n",
    "\tinverse_normalizer = lambda data: np.dot(V,data*stds) + x_means\n",
    "\n",
    "\t# return normalizer\n",
    "\treturn normalizer,inverse_normalizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standard Normalizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standard_normalizer(x):    \n",
    "    # compute the mean and standard deviation of the input\n",
    "    x_means = np.nanmean(x,axis=0)[:,np.newaxis]\n",
    "    x_stds = np.nanstd(x,axis=0, dtype=float)[:,np.newaxis]   \n",
    "\n",
    "    # check to make sure thta x_stds > small threshold, for those not\n",
    "    # divide by 1 instead of original standard deviation\n",
    "    ind = np.argwhere(x_stds < 10**(-2))\n",
    "    if len(ind) > 0:\n",
    "        ind = [v[0] for v in ind]\n",
    "        adjust = np.zeros((x_stds.shape))\n",
    "        adjust[ind] = 1.0\n",
    "        x_stds += adjust\n",
    "\n",
    "    # fill in any nan values with means \n",
    "    ind = np.argwhere(np.isnan(x) == True)\n",
    "    for i in ind:\n",
    "        x[i[0],i[1]] = x_means[i[0]]\n",
    "\n",
    "    # create standard normalizer function\n",
    "    normalizer = lambda data: (data - x_means)/x_stds\n",
    "\n",
    "    # create inverse standard normalizer\n",
    "    inverse_normalizer = lambda data: data*x_stds + x_means\n",
    "\n",
    "    # return normalizer \n",
    "    return normalizer,inverse_normalizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(pred, actual):\n",
    "    df = pd.DataFrame({'Predicted': pred, 'Actual': actual})\n",
    "    df = df.reset_index()\n",
    "\n",
    "    correct = 0\n",
    "    for index, row in df.iterrows():\n",
    "        if row['Predicted'] == row['Actual']:\n",
    "            correct += 1\n",
    "\n",
    "    return correct / float(len(df.index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing different methods of pre-processing\n",
    "Original data, standard normalization, and PCA normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Data\n",
      "Kernel: 0.8032402234636871\n",
      "NN:     0.7986033519553075\n",
      "Tree:   0.7986033519553075\n",
      "\n",
      "Standard Normalized\n",
      "Kernel: 0.8032960893854749\n",
      "NN:     0.7995530726256987\n",
      "Tree:   0.7995530726256987\n",
      "\n",
      "PCA Normalized\n",
      "Kernel: 0.7985474860335197\n",
      "NN:     0.8012290502793298\n",
      "Tree:   0.8012290502793298\n",
      "\n"
     ]
    }
   ],
   "source": [
    "loops = 100\n",
    "\n",
    "for normalizer in range(3):\n",
    "    total_acc = [0,0,0]\n",
    "    if normalizer == 0:\n",
    "        print(\"Original Data\")\n",
    "    if normalizer == 1:\n",
    "        print(\"Standard Normalized\")\n",
    "    if normalizer == 2:\n",
    "        print(\"PCA Normalized\")\n",
    "\n",
    "    for i in range(loops):\n",
    "        # These are the features that we want to use for training,\n",
    "        # since some of them would not be relevant or contain incomplete data\n",
    "        features = [\"Pclass\", \"Sex\", \"SibSp\", \"Parch\"]\n",
    "\n",
    "        # Every loop, we change the random_state so that splits are random and reproducible\n",
    "        train_split, test_split = train_test_split(data, test_size=0.2, random_state=i)\n",
    "        \n",
    "        # get_dummies assigns numerical values to non-numerical data\n",
    "        x_train = pd.get_dummies(train_split[features])\n",
    "        x_test = pd.get_dummies(test_split[features])\n",
    "\n",
    "        # Standard Normalization\n",
    "        if normalizer == 1:\n",
    "            std, rev_std = standard_normalizer(x_train)\n",
    "            x_train = std(np.array(x_train).T).T\n",
    "            x_test = std(np.array(x_test).T).T\n",
    "\n",
    "        # PCA Normalization\n",
    "        if normalizer == 2:\n",
    "            std, rev_std = PCA_sphereing(np.array(x_train, dtype=float).T)\n",
    "            x_train = std(np.array(x_train).T).T\n",
    "            x_test = std(np.array(x_test).T).T\n",
    "\n",
    "        y_train = np.array(train_split['Survived'])\n",
    "        y_test = np.array(test_split['Survived'])\n",
    "\n",
    "        ### Model 1: Kernel-based method\n",
    "        model1 = svm.SVC(kernel='rbf')\n",
    "        model1.fit(x_train, y_train)\n",
    "        pred1 = model1.predict(x_test)\n",
    "\n",
    "        model1_accuracy = accuracy(pred1, y_test)\n",
    "        total_acc[0] += model1_accuracy\n",
    "\n",
    "        ### Model 2: Neural Network\n",
    "        model2 = MLPClassifier(hidden_layer_sizes=(25,), alpha=0.001, max_iter=1000, random_state=0)\n",
    "        model2.fit(x_train, y_train)\n",
    "        pred2 = model2.predict(x_test)\n",
    "\n",
    "        model2_accuracy = accuracy(pred2, y_test)\n",
    "        total_acc[1] += model2_accuracy\n",
    "\n",
    "        # Model 3: Decision Tree\n",
    "        model3 = DecisionTreeClassifier()\n",
    "        model3.fit(x_train, y_train)\n",
    "        pred3 = model2.predict(x_test)\n",
    "\n",
    "        model3_accuracy = accuracy(pred3, y_test)\n",
    "        total_acc[2] += model3_accuracy\n",
    "\n",
    "    print(\"Kernel:\", total_acc[0] / loops)\n",
    "    print(\"NN:    \", total_acc[1] / loops)\n",
    "    print(\"Tree:  \", total_acc[2] / loops)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accuracies are very similar, so we will pick original data as it requires the least amount of processing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-Fold Validation on NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Validation Accuracy: 0.8160112359550561\n",
      "Best Hyperparameters: [0.001, 0.001]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Loop through different hyperparameters, saving the ones with lowest validation cost\n",
    "best_accuracy = 0\n",
    "best_hyperparams = []\n",
    "\n",
    "num_folds = 4\n",
    "folds = split_data(y_train, num_folds)\n",
    "\n",
    "for exp1 in range(3, 8):\n",
    "    alpha = 10**(-exp1)\n",
    "    for exp2 in range(3, 8):\n",
    "        lam = 10**(-exp2)\n",
    "        lam = 10**(-exp2)\n",
    "        total_val_accuracy = 0\n",
    "        # print(f\"Testing alpha={alpha}, lambda={lam}\")\n",
    "\n",
    "        for i in range(num_folds):\n",
    "            # run gradient descent with current alpha and lam\n",
    "            # on every fold except i\n",
    "            current_train_x = []\n",
    "            current_train_y = []\n",
    "\n",
    "            current_val_x = []\n",
    "            current_val_y = []\n",
    "\n",
    "            for j, f in enumerate(folds):\n",
    "                if int(f) - 1 == i:\n",
    "                    current_val_x.append(x_train[j])\n",
    "                    current_val_y.append(y_train[j])\n",
    "                else:\n",
    "                    current_train_x.append(x_train[j])\n",
    "                    current_train_y.append(y_train[j])\n",
    "            \n",
    "            current_train_x = np.array(current_train_x)\n",
    "            current_train_y = np.array(current_train_y)\n",
    "\n",
    "            current_val_x = np.array(current_val_x)\n",
    "            current_val_y = np.array(current_val_y)\n",
    "\n",
    "            warnings.simplefilter('ignore', category=ConvergenceWarning)\n",
    "            model2 = MLPClassifier(hidden_layer_sizes=(25,), learning_rate_init=alpha, alpha=lam, max_iter=1000, random_state=0)\n",
    "            model2.fit(current_train_x, current_train_y)\n",
    "            pred2 = model2.predict(current_val_x)\n",
    "\n",
    "            # calculate accuracy of fold i using predicted values\n",
    "            total_val_accuracy += accuracy(pred2, current_val_y)\n",
    "        \n",
    "        # if better than current best, save\n",
    "        avg_val_accuracy = total_val_accuracy / float(num_folds)\n",
    "\n",
    "        if avg_val_accuracy > best_accuracy:\n",
    "            best_accuracy = avg_val_accuracy\n",
    "            best_hyperparams = [alpha, lam]\n",
    "\n",
    "print(f\"Best Validation Accuracy: {best_accuracy}\")\n",
    "print(f\"Best Hyperparameters: {best_hyperparams}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optimal hyperparameters:\n",
    "- Learning Rate: 0.001\n",
    "- Regularization Parameter: 0.001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test the best hyperparameters determined by k-fold validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.770949720670391\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 0.001\n",
    "alpha = 0.001\n",
    "warnings.simplefilter('ignore', category=ConvergenceWarning)\n",
    "model2 = MLPClassifier(hidden_layer_sizes=(25,), learning_rate_init=learning_rate, alpha=alpha, max_iter=1000, random_state=0)\n",
    "model2.fit(x_train, y_train)\n",
    "pred2 = model2.predict(x_test)\n",
    "print(accuracy(pred2, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test if adding or removing features improves the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Sex']\n",
      "0.7875977653631288\n",
      "['Sex', 'SibSp']\n",
      "0.7963873370577284\n",
      "['Sex', 'SibSp', 'Parch']\n",
      "0.7972439478584732\n",
      "['Sex', 'SibSp', 'Parch', 'Embarked']\n",
      "0.8013966480446931\n",
      "['Sex', 'SibSp', 'Parch', 'Embarked', 'Pclass']\n",
      "0.8025512104283056\n",
      "['Sex', 'SibSp', 'Parch', 'Embarked', 'Pclass', 'Fare']\n",
      "0.8004469273743017\n"
     ]
    }
   ],
   "source": [
    "features_list = [\n",
    "    \"Pclass\",\n",
    "    \"Sex\",\n",
    "    \"SibSp\",\n",
    "    \"Parch\",\n",
    "    \"Fare\",\n",
    "    \"Embarked\"\n",
    "]\n",
    "\n",
    "loops = 100\n",
    "top_features = []\n",
    "\n",
    "for _ in range(6):\n",
    "\n",
    "    best_feature = \"\"\n",
    "    best_accuracy = 0\n",
    "\n",
    "    for feature in features_list:\n",
    "        total_acc = [0,0,0]\n",
    "\n",
    "        if feature in top_features:\n",
    "            continue\n",
    "\n",
    "        features = top_features + [feature]\n",
    "\n",
    "        for i in range(loops):\n",
    "            # Every loop, we change the random_state so that splits are random and reproducible\n",
    "            train_split, test_split = train_test_split(data, test_size=0.2, random_state=i)\n",
    "            # print(train_split.shape)\n",
    "            \n",
    "            # get_dummies assigns numerical values to non-numerical data\n",
    "            x_train = pd.get_dummies(train_split[features])\n",
    "            x_test = pd.get_dummies(test_split[features])\n",
    "            # print(x_train.shape)\n",
    "\n",
    "            # Standard Normalization\n",
    "            if normalizer == 1:\n",
    "                std, rev_std = standard_normalizer(x_train)\n",
    "                x_train = std(np.array(x_train).T).T\n",
    "                x_test = std(np.array(x_test).T).T\n",
    "\n",
    "            # PCA Normalization\n",
    "            if normalizer == 2:\n",
    "                std, rev_std = PCA_sphereing(np.array(x_train, dtype=float).T)\n",
    "                x_train = std(np.array(x_train).T).T\n",
    "                x_test = std(np.array(x_test).T).T\n",
    "\n",
    "            y_train = np.array(train_split['Survived'])\n",
    "            y_test = np.array(test_split['Survived'])\n",
    "\n",
    "            ### Model 1: Kernel-based method\n",
    "            model1 = svm.SVC(kernel='rbf')\n",
    "            model1.fit(x_train, y_train)\n",
    "            pred1 = model1.predict(x_test)\n",
    "\n",
    "            model1_accuracy = accuracy(pred1, y_test)\n",
    "            total_acc[0] += model1_accuracy\n",
    "\n",
    "            ### Model 2: Neural Network\n",
    "            model2 = MLPClassifier(hidden_layer_sizes=(25,), alpha=0.001, max_iter=1000, random_state=0)\n",
    "            model2.fit(x_train, y_train)\n",
    "            pred2 = model2.predict(x_test)\n",
    "\n",
    "            model2_accuracy = accuracy(pred2, y_test)\n",
    "            total_acc[1] += model2_accuracy\n",
    "\n",
    "            # Model 3: Decision Tree\n",
    "            model3 = DecisionTreeClassifier()\n",
    "            model3.fit(x_train, y_train)\n",
    "            pred3 = model2.predict(x_test)\n",
    "\n",
    "            model3_accuracy = accuracy(pred3, y_test)\n",
    "            total_acc[2] += model3_accuracy\n",
    "\n",
    "        avg_acc = sum(total_acc) / (3 * loops)\n",
    "        if avg_acc > best_accuracy:\n",
    "            best_accuracy = avg_acc\n",
    "            best_feature = feature\n",
    "\n",
    "    top_features.append(best_feature)\n",
    "    print(top_features)\n",
    "    print(best_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
